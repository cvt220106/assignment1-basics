import collections
import regex as re
import multiprocessing
import os
from typing import BinaryIO, Counter as CounterType, Tuple, Dict, List

try:
    from rust_bpe_optimizer import merge_word_counts_and_update_stats_rust
    RUST_AVAILABLE = True
    print("ğŸš€ Successfully imported Rust optimizer!")
except ImportError:
    RUST_AVAILABLE = False
    print("âš ï¸ Could not import Rust optimizer. Falling back to pure Python version.")

# GPT-2 çš„é¢„åˆ†è¯æ­£åˆ™è¡¨è¾¾å¼
# è¯¦ç»†è§£é‡Š:
# 's|'t|'re|'ve|'m|'ll|'d : åŒ¹é…å¸¸è§çš„ç¼©å†™
# ?\p{L}+ : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ª Unicode å­—æ¯ (ä¾‹å¦‚ "hello", "ä½ å¥½")
# ?\p{N}+ : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ª Unicode æ•°å­— (ä¾‹å¦‚ "123")
# ?[^\s\p{L}\p{N}]+ : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªéç©ºæ ¼ã€éå­—æ¯ã€éæ•°å­—çš„å­—ç¬¦ (ä¾‹å¦‚ ".,?!")
# \s+(?!\S) : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼ï¼Œä½†åªåœ¨å®ƒåé¢æ²¡æœ‰éç©ºæ ¼å­—ç¬¦æ—¶ (å¤„ç†è¡Œå°¾ç©ºæ ¼)
# \s+ : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼
GPT2_PATTERN = re.compile(r"'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+")


def find_chunk_boundaries(
        file: BinaryIO,
        desired_num_chunks: int,
        split_special_token: bytes,
) -> list[int]:
    """
    å‡½æ•°è¯´æ˜:
        åœ¨ä¸€ä¸ªå¤§æ–‡ä»¶ä¸­æ‰¾åˆ°å®‰å…¨çš„åˆ†å—è¾¹ç•Œï¼Œä»¥åˆ›å»ºæœŸæœ›æ•°é‡çš„å—ã€‚
        æ¯ä¸ªè¾¹ç•Œéƒ½ä½äºæŒ‡å®šç‰¹æ®Šæ ‡è®°çš„å¼€å¤´ï¼Œä»¥ç¡®ä¿é€»è¾‘å•å…ƒï¼ˆå¦‚æ–‡æ¡£ï¼‰ä¸ä¼šè¢«åˆ‡åˆ†ã€‚
        æ­¤å‡½æ•°ä¸ºå¹¶è¡Œå¤„ç†ä¸­çš„è´Ÿè½½å‡è¡¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚

    è¾“å…¥:
        file (BinaryIO): ç”¨äºè¯»å–çš„äºŒè¿›åˆ¶æ–‡ä»¶å¥æŸ„ã€‚
        desired_num_chunks (int): ç›®æ ‡å—æ•°é‡ã€‚å®é™…æ•°é‡å¯èƒ½å› æ ‡è®°åˆ†å¸ƒè€Œå‡å°‘ã€‚
        split_special_token (bytes): æ ‡è®°å®‰å…¨è¾¹ç•Œçš„ç‰¹æ®Šæ ‡è®°ï¼ˆå­—èŠ‚å½¢å¼ï¼‰ï¼Œä¾‹å¦‚ b"<|endoftext|>"ã€‚

    è¾“å‡º:
        list[int]: ä¸€ä¸ªæ’åºåçš„æ–‡ä»¶å†…å­—èŠ‚åç§»é‡åˆ—è¡¨ï¼Œä»£è¡¨æ¯ä¸ªå—çš„å¼€å§‹å’Œç»“æŸã€‚
                   åˆ—è¡¨åŒ…å« 0 å’Œæ–‡ä»¶æ€»å¤§å°ã€‚
    """
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)

    if file_size == 0:
        return [0, 0]

    chunk_size = file_size // desired_num_chunks
    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks)]
    chunk_boundaries.append(file_size)

    # Refine boundaries to align with the special token
    mini_chunk_size = 4096  # Read 4KB at a time to find the token
    for i in range(1, len(chunk_boundaries) - 1):
        position = chunk_boundaries[i]
        file.seek(position)
        while True:
            mini_chunk = file.read(mini_chunk_size)
            if not mini_chunk:
                position = file_size  # Reached end of file
                break

            found_at = mini_chunk.find(split_special_token)
            if found_at != -1:
                position += found_at
                break
            position += len(mini_chunk)
        chunk_boundaries[i] = position

    # Add the start boundary and ensure uniqueness and order
    final_boundaries = sorted(list(set([0] + chunk_boundaries)))
    return final_boundaries


def process_chunk_for_counts(
        chunk_data: tuple[bytes, str, re.Pattern]
) -> CounterType[tuple[int, ...]]:
    """
    å‡½æ•°è¯´æ˜:
        ä¸€ä¸ªç”¨äºå¤šè¿›ç¨‹çš„å·¥ä½œå‡½æ•°ï¼Œå¤„ç†å•ä¸ªæ–‡æœ¬å—ã€‚
        å®ƒé¦–å…ˆæ ¹æ®ç‰¹æ®Šæ ‡è®°åˆ†å‰²æ–‡æœ¬å—ï¼Œç„¶åç”¨æ­£åˆ™è¡¨è¾¾å¼å¯¹å­å—è¿›è¡Œé¢„åˆ†è¯ï¼Œ
        æœ€åè¿”å›æ¯ä¸ªç‹¬ä¸€æ— äºŒçš„â€œå•è¯â€çš„é¢‘ç‡è®¡æ•°ã€‚

    è¾“å…¥:
        chunk_data (tuple): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«åŸå§‹æ–‡æœ¬å—å­—èŠ‚ã€ç”¨äºåˆ†å‰²ç‰¹æ®Šæ ‡è®°çš„
                            æ­£åˆ™è¡¨è¾¾å¼ï¼Œä»¥åŠ BPE é¢„åˆ†è¯çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚

    è¾“å‡º:
        CounterType[tuple[int, ...]]: ä¸€ä¸ªè®¡æ•°å™¨ï¼Œé”®æ˜¯é¢„åˆ†è¯åçš„â€œå•è¯â€ï¼ˆä»¥å­—èŠ‚å€¼çš„å…ƒç»„è¡¨ç¤ºï¼‰ï¼Œ
                                      å€¼æ˜¯å®ƒä»¬çš„å‡ºç°é¢‘ç‡ã€‚
    """
    chunk_bytes, special_tokens_pattern, regex_pattern = chunk_data
    text = chunk_bytes.decode('utf-8', errors='replace')

    word_counts = collections.Counter()
    # First, split the chunk by ALL special tokens to prevent merges across them
    sub_chunks = re.split(special_tokens_pattern, text)

    for sub_chunk in sub_chunks:
        if sub_chunk:
            for match in regex_pattern.finditer(sub_chunk):
                word_bytes = match.group(0).encode('utf-8')
                word_counts[tuple(word_bytes)] += 1
    return word_counts


def get_stats_from_word_counts(
        word_counts: CounterType[tuple[int, ...]]
) -> CounterType[tuple[int, int]]:
    """
    å‡½æ•°è¯´æ˜:
        ä»â€œå•è¯â€é¢‘ç‡è®¡æ•°å™¨ä¸­é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„åˆå§‹é¢‘ç‡ã€‚
        æ­¤æ–¹æ³•æ“ä½œçš„æ˜¯å”¯ä¸€è¯ï¼Œå¹¶é€šè¿‡ä¹˜ä»¥å…¶é¢‘ç‡æ¥è®¡ç®—ï¼Œè¿œæ¯”éå†æ•´ä¸ªè¯­æ–™åº“é«˜æ•ˆã€‚

    è¾“å…¥:
        word_counts (CounterType): å”¯ä¸€è¯åŠå…¶é¢‘ç‡çš„è®¡æ•°å™¨ã€‚

    è¾“å‡º:
        CounterType[tuple[int, int]]: ç›¸é‚»å­—èŠ‚å¯¹åŠå…¶åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­æ€»é¢‘ç‡çš„è®¡æ•°å™¨ã€‚
    """
    pair_stats = collections.Counter()
    for word_tuple, count in word_counts.items():
        for pair in zip(word_tuple[:-1], word_tuple[1:]):
            pair_stats[pair] += count
    return pair_stats


def merge_word_counts_and_update_stats(
        word_counts: CounterType[Tuple[int, ...]],
        pair_to_merge: Tuple[int, int],
        new_token_id: int
) -> Tuple[CounterType[Tuple[int, ...]], CounterType[Tuple[int, int]]]:
    """
    å‡½æ•°è¯´æ˜:
        åœ¨è¯é¢‘è®¡æ•°å™¨ä¸Šæ‰§è¡Œ BPE åˆå¹¶æ“ä½œï¼Œå¹¶ä¸€æ­¥è®¡ç®—å‡ºè¯å¯¹é¢‘ç‡çš„å˜åŒ–ã€‚

    è¾“å…¥:
        word_counts (CounterType): å½“å‰çš„è¯é¢‘è®¡æ•°å™¨ã€‚
        pair_to_merge (Tuple[int, int]): éœ€è¦è¢«åˆå¹¶çš„å­—èŠ‚å¯¹ï¼ˆä¾‹å¦‚ (116, 104) ä»£è¡¨ 'th'ï¼‰ã€‚
        new_token_id (int): åˆå¹¶åäº§ç”Ÿçš„æ–°è¯å…ƒ IDã€‚

    è¾“å‡º:
        Tuple[CounterType, CounterType]: ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ï¼š
        - new_word_counts: åˆå¹¶åæ›´æ–°çš„è¯é¢‘è®¡æ•°å™¨ã€‚
        - stats_delta: ä¸€ä¸ªä»£è¡¨è¯å¯¹é¢‘ç‡å˜åŒ–é‡ï¼ˆå¢é‡ï¼‰çš„è®¡æ•°å™¨ï¼Œä¾›ä¸»å¾ªç¯åº”ç”¨ã€‚
    """

    new_word_counts = collections.Counter()
    stats_delta = collections.Counter()
    p1, p2 = pair_to_merge

    for word, count in word_counts.items():
        if len(word) < 2:
            new_word_counts[word] += count
            continue

        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:
                # This is the merge point. Update stats based on neighbors in the ORIGINAL word.
                # 1. Decrement counts for pairs being destroyed.
                if i > 0:
                    stats_delta[(word[i - 1], p1)] -= count
                if i < len(word) - 2:
                    stats_delta[(p2, word[i + 2])] -= count

                # 2. Increment counts for new pairs being formed.
                if i > 0:
                    stats_delta[(word[i - 1], new_token_id)] += count
                if i < len(word) - 2:
                    stats_delta[(new_token_id, word[i + 2])] += count

                new_word.append(new_token_id)
                i += 2
            else:
                new_word.append(word[i])
                i += 1

        new_word_tuple = tuple(new_word)
        new_word_counts[new_word_tuple] += count

    return new_word_counts, stats_delta


def train_bpe(
        input_path: str,
        vocab_size: int,
        special_tokens: list[str]
) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:
    """
    å‡½æ•°è¯´æ˜:
        ä»ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶è®­ç»ƒä¸€ä¸ªå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰åˆ†è¯å™¨ã€‚
        æ­¤å®ç°ç»è¿‡é«˜åº¦ä¼˜åŒ–ï¼Œä½¿ç”¨äº†å¹¶è¡Œé¢„åˆ†è¯å’ŒåŸºäºè¯é¢‘çš„é«˜æ•ˆåˆå¹¶ç­–ç•¥ã€‚

    è¾“å…¥:
        input_path (str): è®­ç»ƒæ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ã€‚
        vocab_size (int): æœŸæœ›çš„æœ€ç»ˆè¯æ±‡è¡¨å¤§å°ã€‚
        special_tokens (list[str]): éœ€è¦æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚
                                    ç¬¬ä¸€ä¸ªç‰¹æ®Šæ ‡è®°å°†è¢«ç”¨ä½œæ–‡æ¡£åˆ†éš”ç¬¦ä»¥è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚

    è¾“å‡º:
        tuple: ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ï¼š
        - vocab (dict[int, bytes]): æœ€ç»ˆçš„è¯æ±‡è¡¨ï¼Œæ˜ å°„ token ID åˆ°å­—èŠ‚ã€‚
        - merges (list[tuple[bytes, bytes]]): æŒ‰åˆ›å»ºé¡ºåºåˆ—å‡ºçš„åˆå¹¶è§„åˆ™åˆ—è¡¨ã€‚
    """
    assert vocab_size >= 256 + len(special_tokens), "Vocab size is too small."

    # --- 1. Vocabulary Initialization ---
    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}
    for i, token_str in enumerate(special_tokens):
        vocab[256 + i] = token_str.encode('utf-8')

    # --- 2. Parallel Pre-tokenization ---
    print("Starting parallel pre-tokenization to get word counts...")
    special_tokens_pattern = "|".join(re.escape(st) for st in special_tokens)
    doc_end_token = b"<|endoftext|>"
    num_processes = 16

    chunks_data = []
    with open(input_path, "rb") as f:
        boundaries = find_chunk_boundaries(f, num_processes, doc_end_token)
        print(f"File split into {len(boundaries) - 1} chunks for {num_processes} processes.")
        for start, end in zip(boundaries, boundaries[1:]):
            if start < end:
                f.seek(start)
                chunks_data.append((f.read(end - start), special_tokens_pattern, GPT2_PATTERN))

    word_counts = collections.Counter()
    with multiprocessing.Pool(num_processes) as pool:
        results = pool.map(process_chunk_for_counts, chunks_data)
        for res in results:
            word_counts.update(res)

    print(f"Pre-tokenization complete. Found {len(word_counts)} unique 'words'.")

    # --- 3. BPE Training Loop (Optimized) ---
    merges: List[Tuple[bytes, bytes]] = []
    stats = get_stats_from_word_counts(word_counts)

    num_merges_needed = vocab_size - len(vocab)
    for i in range(num_merges_needed):
        if not stats:
            print("No more pairs to merge. Stopping early.")
            break

        max_freq = max(stats.values())

        if max_freq < 1:
            print("Highest pair count is 0. Stopping early.")
            break

        tied_pairs = [p for p, freq in stats.items() if freq == max_freq]
        best_pair = max(tied_pairs, key=lambda p: (vocab[p[0]], vocab[p[1]]))

        new_token_id = len(vocab)
        token1_bytes = vocab[best_pair[0]]
        token2_bytes = vocab[best_pair[1]]
        vocab[new_token_id] = token1_bytes + token2_bytes
        merges.append((token1_bytes, token2_bytes))

        if RUST_AVAILABLE:
            # è°ƒç”¨ Rust å‡½æ•°ï¼Œå®ƒè¿”å›çš„æ˜¯ (dict, dict)
            new_counts_dict, delta_dict = merge_word_counts_and_update_stats_rust(
                word_counts, best_pair, new_token_id
            )

            # æˆ‘ä»¬éœ€è¦å°†è¿”å›çš„ dict è½¬æ¢ä¸º Counter
            word_counts = collections.Counter(new_counts_dict)
            stats_delta = collections.Counter(delta_dict)
        else:
            # è°ƒç”¨çº¯ Python å‡½æ•°
            word_counts, stats_delta = merge_word_counts_and_update_stats(
                word_counts, best_pair, new_token_id
            )

        stats.update(stats_delta)
        del stats[best_pair]

        if (i + 1) % 100 == 0:
            print(
                f"Merge {i + 1}/{num_merges_needed}: {best_pair} -> {new_token_id} ({vocab[new_token_id]!r}) | Freq: {stats[max(stats, key=stats.get)] if stats else 0}")

    return vocab, merges


if __name__ == '__main__':
    # This block allows the script to be run directly for a demonstration.

    # 1. Create a dummy data file for training.
#     sample_text = """Hello world.\nThis is a sample text for BPE training.\nIt repeats some words, like sample text.
# <|endoftext|>
# This is the second document. It also has sample text.
# The BPE algorithm will learn to merge common character pairs.
# For example, 's' and 'a', then 'sa' and 'm', and so on.
# <|endoftext|>
# The third document is here to provide more data."""
#     sample_text = "low low low low low lower lower widest widest widest newest newest newest newest newest newest and the vocabulary has a special token <|endoftext|>"
#
#     input_file_path = "data.txt"
#     with open(input_file_path, "w", encoding="utf-8") as f:
#         f.write(sample_text)
#
#     print(f"Created sample training data at '{input_file_path}'")

    input_file_path = "../data/TinyStoriesV2-GPT4-train.txt"

    # 2. Define training parameters.
    TARGET_VOCAB_SIZE = 1000
    SPECIAL_TOKENS = ["<|endoftext|>"]

    print("\n--- Starting BPE Training ---")

    # 3. Run the training function.
    final_vocab, final_merges = train_bpe(
        input_path=input_file_path,
        vocab_size=TARGET_VOCAB_SIZE,
        special_tokens=SPECIAL_TOKENS
    )

    # 4. Print the results.
    print("\n--- BPE Training Complete ---")

    print(f"\nFinal Vocabulary Size: {len(final_vocab)}")
    print("--- First 10 and all new tokens ---")
    for token_id, token_bytes in sorted(final_vocab.items()):
        if token_id < 10 or token_id >= 256:
            try:
                # Attempt to decode for readability, show bytes on failure
                decoded = token_bytes.decode('utf-8')
                print(f"{token_id}: {decoded!r}  ({token_bytes})")
            except UnicodeDecodeError:
                print(f"{token_id}: {token_bytes}")

    print("\n--- Merge Rules ---")
    for i, pair in enumerate(final_merges):
        p1_decoded = pair[0].decode('utf-8', errors='replace')
        p2_decoded = pair[1].decode('utf-8', errors='replace')
        print(f"{i + 1:02d}: {p1_decoded!r} + {p2_decoded!r} -> {(p1_decoded + p2_decoded)!r}")